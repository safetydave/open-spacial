import os
import sys
from scipy.stats import norm
import unittest

sys.path.append(os.path.abspath('..'))
sys.path.append(os.path.abspath('../src'))
from src.trialrunner import TrialRunner


class TestTrialRunner(unittest.TestCase):

    def setUp(self):
        # (mu, std) for normal distribution approximations to results over many runs
        # used trials = 10000 for each run
        # aligned to ps = [x / 10.0 for x in range(11)]; ps.reverse()
        self.ds = [(0.0, 0.0),
                   (0.0, 0.0),
                   (0.0, 0.0),
                   (0.00011550000000000002, 9.648704576263075e-05),
                   (0.0052970000000000005, 0.0006884700429212587),
                   (0.0607685, 0.0022697043309647183),
                   (0.3629765, 0.00466831851419759),
                   (0.8036955, 0.003776086565480197),
                   (0.9665659999999999, 0.0016893916064666665),
                   (0.9964065, 0.0005659573747200441),
                   (1.0, 0.0)]

    def test_run_summary(self):
        tr = TrialRunner(10, [1.0])
        tr.run()
        self.assertEqual("Number of samples for each p: 10\n1.0 0.000\n", tr.summary())

    # test null hypothesis that a result came from the distribution
    def p_val_result_from_distn(self, result, distn):
        p_val = 1.0
        if distn[1] == 0.0:
            if result != distn[0]:
                p_val = 0.0
        else:
            z = result - distn[0]
            p_val = 2 * norm.sf(abs(z), scale=distn[1])
        return p_val

    # accept the null hypothesis at some significance level
    def accept_r_d(self, result, distn, sig=0.05):
        return self.p_val_result_from_distn(result, distn) > sig

    # accept a set of 7 (where sd > 0) results unless 99% sure we should reject the set (per result sig=0.0015)
    def accept_all_results_99(self, rs):
        # nb. we can make this fail for instance (probably) at sig=0.1
        return len(rs) == sum([self.accept_r_d(r, self.ds[i], sig=0.0015) for i, r in enumerate(rs)])

    # test this run is likely to come from the expected distribution
    # nb. this is a bit circular when the distribution was generated by the same process
    # but if either process or distribution data changes, this test can pick it up
    def test_run_distribution(self):
        trials = 10000
        # occupancy proportions to test
        ps = [x / 10.0 for x in range(11)]
        ps.reverse()

        tr = TrialRunner(trials, ps)
        tr.run()
        self.assertTrue(self.accept_all_results_99(tr.results))


if __name__ == '__main__':
    unittest.main()
